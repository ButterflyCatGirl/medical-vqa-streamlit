# Ultra-Fast Medical VQA Streamlit App - FINAL ACCURATE VERSION
import streamlit as st
from PIL import Image, ImageOps
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import logging
import time
import gc
from typing import Optional, Dict, Any
import warnings
import re

# Suppress warnings
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Optimized Configuration
MAX_IMAGE_SIZE = (384, 384)  # Better for medical images
SUPPORTED_FORMATS = ["jpg", "jpeg", "png"]
MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB
FINE_TUNED_MODEL = "sharawy53/blip-vqa-medical-arabic"

class AccurateMedicalVQA:
    """Accurate Medical VQA System with Enhanced Responses"""
    
    def __init__(self):
        self.processor = None
        self.model = None
        self.device = self._get_device()
        self.medical_terms = self._load_comprehensive_medical_terms()
        
    def _get_device(self) -> str:
        """Get optimal device"""
        if torch.cuda.is_available():
            return "cuda"
        return "cpu"
    
    def _load_comprehensive_medical_terms(self) -> Dict[str, str]:
        """Comprehensive medical terminology for accurate Arabic responses"""
        return {
            # Basic medical terms
            "normal": "Ø·Ø¨ÙŠØ¹ÙŠ", "abnormal": "ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ", "healthy": "Ø³Ù„ÙŠÙ…",
            "disease": "Ù…Ø±Ø¶", "condition": "Ø­Ø§Ù„Ø©", "patient": "Ù…Ø±ÙŠØ¶",
            
            # Body parts and anatomy
            "chest": "Ø§Ù„ØµØ¯Ø±", "lung": "Ø§Ù„Ø±Ø¦Ø©", "lungs": "Ø§Ù„Ø±Ø¦ØªØ§Ù†", "heart": "Ø§Ù„Ù‚Ù„Ø¨",
            "brain": "Ø§Ù„Ø¯Ù…Ø§Øº", "liver": "Ø§Ù„ÙƒØ¨Ø¯", "kidney": "Ø§Ù„ÙƒÙ„ÙŠØ©", "spine": "Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ",
            "bone": "Ø¹Ø¸Ù…", "bones": "Ø¹Ø¸Ø§Ù…", "skull": "Ø§Ù„Ø¬Ù…Ø¬Ù…Ø©", "rib": "Ø¶Ù„Ø¹", "ribs": "Ø£Ø¶Ù„Ø§Ø¹",
            "abdomen": "Ø§Ù„Ø¨Ø·Ù†", "pelvis": "Ø§Ù„Ø­ÙˆØ¶", "shoulder": "Ø§Ù„ÙƒØªÙ", "neck": "Ø§Ù„Ø±Ù‚Ø¨Ø©",
            
            # Medical imaging
            "x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ©", "ct scan": "ØªØµÙˆÙŠØ± Ù…Ù‚Ø·Ø¹ÙŠ Ù…Ø­ÙˆØ³Ø¨", "mri": "Ø±Ù†ÙŠÙ† Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ",
            "ultrasound": "Ù…ÙˆØ¬Ø§Øª ÙÙˆÙ‚ ØµÙˆØªÙŠØ©", "radiograph": "ØµÙˆØ±Ø© Ø´Ø¹Ø§Ø¹ÙŠØ©", "scan": "ÙØ­Øµ Ø¨Ø§Ù„Ø£Ø´Ø¹Ø©",
            
            # Medical conditions
            "pneumonia": "Ø§Ù„ØªÙ‡Ø§Ø¨ Ø±Ø¦ÙˆÙŠ", "infection": "Ø§Ù„ØªÙ‡Ø§Ø¨", "inflammation": "Ø§Ù„ØªÙ‡Ø§Ø¨",
            "fracture": "ÙƒØ³Ø±", "broken": "Ù…ÙƒØ³ÙˆØ±", "tumor": "ÙˆØ±Ù…", "mass": "ÙƒØªÙ„Ø©",
            "cancer": "Ø³Ø±Ø·Ø§Ù†", "fluid": "Ø³ÙˆØ§Ø¦Ù„", "swelling": "ØªÙˆØ±Ù…", "pain": "Ø£Ù„Ù…",
            
            # Medical observations
            "shows": "ÙŠÙØ¸Ù‡Ø±", "appears": "ÙŠØ¨Ø¯Ùˆ", "indicates": "ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰", "suggests": "ÙŠÙˆØ­ÙŠ Ø¨Ù€",
            "visible": "Ù…Ø±Ø¦ÙŠ", "evident": "ÙˆØ§Ø¶Ø­", "present": "Ù…ÙˆØ¬ÙˆØ¯", "absent": "ØºØ§Ø¦Ø¨",
            "enlarged": "Ù…ØªØ¶Ø®Ù…", "reduced": "Ù…Ù†Ø®ÙØ¶", "increased": "Ù…Ø±ØªÙØ¹", "decreased": "Ù…Ù†Ø®ÙØ¶",
            
            # Medical actions
            "examination": "ÙØ­Øµ", "diagnosis": "ØªØ´Ø®ÙŠØµ", "treatment": "Ø¹Ù„Ø§Ø¬", "surgery": "Ø¬Ø±Ø§Ø­Ø©",
            "consultation": "Ø§Ø³ØªØ´Ø§Ø±Ø©", "follow-up": "Ù…ØªØ§Ø¨Ø¹Ø©", "monitoring": "Ù…Ø±Ø§Ù‚Ø¨Ø©",
            
            # Common phrases
            "what is": "Ù…Ø§ Ù‡Ùˆ", "what are": "Ù…Ø§ Ù‡ÙŠ", "this image": "Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©",
            "medical image": "ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ©", "likely": "Ù…Ø­ØªÙ…Ù„", "possible": "Ù…Ù…ÙƒÙ†"
        }
    
    def _create_medical_prompt(self, question: str) -> str:
        """Create enhanced medical prompt for better responses"""
        medical_prompt_prefix = "As a medical AI assistant analyzing medical images, provide accurate medical observations. "
        
        # Add context based on question type
        if any(word in question.lower() for word in ["diagnosis", "ØªØ´Ø®ÙŠØµ"]):
            return f"{medical_prompt_prefix}Focus on diagnostic findings: {question}"
        elif any(word in question.lower() for word in ["normal", "abnormal", "Ø·Ø¨ÙŠØ¹ÙŠ"]):
            return f"{medical_prompt_prefix}Assess if findings are normal or abnormal: {question}"
        elif any(word in question.lower() for word in ["chest", "lung", "ØµØ¯Ø±", "Ø±Ø¦Ø©"]):
            return f"{medical_prompt_prefix}Analyze chest/pulmonary findings: {question}"
        else:
            return f"{medical_prompt_prefix}{question}"
    
    @st.cache_resource(show_spinner=False)
    def load_model(_self):
        """Load fine-tuned model with caching"""
        try:
            logger.info(f"Loading fine-tuned model: {FINE_TUNED_MODEL}")
            
            # Load with optimizations
            _self.processor = BlipProcessor.from_pretrained(FINE_TUNED_MODEL)
            
            # Set pad token properly
            if _self.processor.tokenizer.pad_token is None:
                _self.processor.tokenizer.pad_token = _self.processor.tokenizer.eos_token
            
            if _self.device == "cpu":
                _self.model = BlipForConditionalGeneration.from_pretrained(
                    FINE_TUNED_MODEL,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True
                )
            else:
                _self.model = BlipForConditionalGeneration.from_pretrained(
                    FINE_TUNED_MODEL,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True
                )
            
            _self.model = _self.model.to(_self.device)
            _self.model.eval()
            
            logger.info(f"Model loaded successfully on {_self.device}")
            return True
            
        except Exception as e:
            logger.error(f"Model loading failed: {str(e)}")
            # Fallback to base model
            try:
                _self.processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
                _self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-vqa-base")
                _self.model = _self.model.to(_self.device)
                logger.info("Fallback to base model successful")
                return True
            except:
                return False
    
    def _detect_language(self, text: str) -> str:
        """Fast language detection"""
        arabic_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
        return "ar" if arabic_chars > 0 else "en"
    
    def _translate_to_arabic_medical(self, text_en: str, question: str = "") -> str:
        """Advanced medical translation to Arabic with context"""
        if not text_en or text_en.strip() == "":
            return "Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©"
        
        # Clean the text first
        text_clean = text_en.strip()
        text_lower = text_clean.lower()
        
        # Medical-specific Arabic responses based on content analysis
        if any(term in text_lower for term in ["normal", "no abnormalities", "healthy", "clear"]):
            if "chest" in text_lower or "lung" in text_lower:
                return "ØªØ¸Ù‡Ø± Ø§Ù„ØµÙˆØ±Ø© Ø±Ø¦ØªÙŠÙ† Ø·Ø¨ÙŠØ¹ÙŠØªÙŠÙ† Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø±Ø¶ÙŠØ© ÙˆØ§Ø¶Ø­Ø©"
            elif "heart" in text_lower:
                return "ÙŠØ¨Ø¯Ùˆ Ø§Ù„Ù‚Ù„Ø¨ Ø·Ø¨ÙŠØ¹ÙŠ Ø§Ù„Ø­Ø¬Ù… ÙˆØ§Ù„Ø´ÙƒÙ„"
            else:
                return "ØªØ¸Ù‡Ø± Ø§Ù„ØµÙˆØ±Ø© Ù†ØªØ§Ø¦Ø¬ Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ø¶Ø­Ø©"
        
        elif any(term in text_lower for term in ["pneumonia", "infection", "infiltrate"]):
            return "ØªØ¸Ù‡Ø± Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© Ù„Ø§Ù„ØªÙ‡Ø§Ø¨ Ø±Ø¦ÙˆÙŠ Ø£Ùˆ Ø¹Ø¯ÙˆÙ‰ ØªØ­ØªØ§Ø¬ Ù„ØªÙ‚ÙŠÙŠÙ… Ø·Ø¨ÙŠ Ù…ØªØ®ØµØµ"
        
        elif any(term in text_lower for term in ["fracture", "break", "broken"]):
            return "ØªØ¸Ù‡Ø± Ø¹Ù„Ø§Ù…Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© Ù„ÙƒØ³Ø± ÙŠØ­ØªØ§Ø¬ Ù„ØªÙ‚ÙŠÙŠÙ… Ø·Ø¨ÙŠ ÙÙˆØ±ÙŠ"
        
        elif any(term in text_lower for term in ["mass", "tumor", "growth"]):
            return "ØªØ¸Ù‡Ø± ÙƒØªÙ„Ø© Ø£Ùˆ Ù†Ù…Ùˆ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ ÙŠØ­ØªØ§Ø¬ Ù„ÙØ­Øµ ÙˆØªÙ‚ÙŠÙŠÙ… Ø·Ø¨ÙŠ Ù…ØªØ®ØµØµ"
        
        elif any(term in text_lower for term in ["fluid", "effusion"]):
            return "ØªØ¸Ù‡Ø± ØªØ¬Ù…Ø¹ Ø³ÙˆØ§Ø¦Ù„ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ ÙŠØ­ØªØ§Ø¬ Ù„ØªÙ‚ÙŠÙŠÙ… Ø·Ø¨ÙŠ"
        
        # Try word-by-word translation for technical terms
        words = text_clean.split()
        translated_words = []
        
        for word in words:
            clean_word = re.sub(r'[^\w\s]', '', word.lower())
            if clean_word in self.medical_terms:
                translated_words.append(self.medical_terms[clean_word])
            else:
                # Check for partial matches
                found = False
                for en_term, ar_term in self.medical_terms.items():
                    if en_term in clean_word or clean_word in en_term:
                        translated_words.append(ar_term)
                        found = True
                        break
                if not found:
                    translated_words.append(word)
        
        result = " ".join(translated_words)
        
        # If translation is still poor, provide contextual medical response
        arabic_char_count = sum(1 for c in result if '\u0600' <= c <= '\u06FF')
        if arabic_char_count < 3:
            return "ØªØ­ØªØ§Ø¬ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø·Ø¨ÙŠØ© Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙˆØªÙØ³ÙŠØ± Ù…Ù† Ù‚Ø¨Ù„ Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ ÙÙŠ Ø§Ù„Ø£Ø´Ø¹Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ´Ø®ÙŠØµ Ø¯Ù‚ÙŠÙ‚"
        
        return result
    
    def _process_image_optimized(self, image: Image.Image) -> Image.Image:
        """Optimized image processing for medical images"""
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Resize maintaining aspect ratio for better medical detail
        if image.size != MAX_IMAGE_SIZE:
            image = ImageOps.fit(image, MAX_IMAGE_SIZE, Image.Resampling.LANCZOS)
        
        return image
    
    def _clean_generated_answer(self, raw_answer: str, original_question: str) -> str:
        """Intelligently clean the generated answer"""
        if not raw_answer:
            return ""
        
        # Remove question only if it appears at the beginning
        answer = raw_answer.strip()
        
        # Check if question appears at start of answer
        question_lower = original_question.lower().strip()
        answer_lower = answer.lower()
        
        if answer_lower.startswith(question_lower):
            # Remove question from beginning
            answer = answer[len(original_question):].strip()
            # Remove common prefixes that might remain
            answer = re.sub(r'^[,\.\:\?\!]+\s*', '', answer)
        
        # Remove common VQA artifacts
        answer = re.sub(r'^(answer|response|result)[\:\s]+', '', answer, flags=re.IGNORECASE)
        
        return answer.strip()
    
    def process_query(self, image: Image.Image, question: str) -> Dict[str, Any]:
        """Process query with enhanced accuracy"""
        try:
            start_time = time.time()
            
            # Process image
            image = self._process_image_optimized(image)
            
            # Detect language
            detected_lang = self._detect_language(question)
            
            # Create enhanced medical prompt
            enhanced_question = self._create_medical_prompt(question)
            
            # Process with model
            inputs = self.processor(image, enhanced_question, return_tensors="pt").to(self.device)
            
            # Generate with improved parameters
            with torch.no_grad():
                if self.device == "cuda":
                    with torch.cuda.amp.autocast():
                        generated_ids = self.model.generate(
                            **inputs,
                            max_length=128,  # Increased for detailed responses
                            num_beams=5,     # More beams for better quality
                            early_stopping=True,
                            do_sample=True,  # Enable sampling
                            temperature=0.7, # Controlled randomness
                            top_p=0.9,      # Nucleus sampling
                            repetition_penalty=1.1  # Avoid repetition
                        )
                else:
                    generated_ids = self.model.generate(
                        **inputs,
                        max_length=128,
                        num_beams=5,
                        early_stopping=True,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        repetition_penalty=1.1
                    )
            
            # Decode and clean answer
            raw_answer = self.processor.decode(generated_ids[0], skip_special_tokens=True)
            answer_en = self._clean_generated_answer(raw_answer, enhanced_question)
            
            # Handle empty or poor answers
            if not answer_en or len(answer_en) < 5:
                answer_en = "Unable to provide a clear medical analysis from this image. Please consult a healthcare professional."
            
            # Generate Arabic response
            if detected_lang == "ar":
                answer_ar = self._translate_to_arabic_medical(answer_en, question)
            else:
                answer_ar = self._translate_to_arabic_medical(answer_en, question)
            
            processing_time = time.time() - start_time
            
            return {
                "question": question,
                "answer_en": answer_en,
                "answer_ar": answer_ar,
                "detected_language": detected_lang,
                "processing_time": processing_time,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Query processing failed: {str(e)}")
            return {
                "error": str(e),
                "success": False
            }

# Streamlit Configuration
def init_app():
    """Initialize app with optimized settings"""
    st.set_page_config(
        page_title="Accurate Medical AI",
        layout="wide",
        page_icon="ğŸ©º"
    )

def apply_theme():
    """Apply enhanced theme"""
    st.markdown("""
    <style>
        .main-header {
            background: linear-gradient(135deg, #2E8B57 0%, #228B22 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin-bottom: 1.5rem;
            text-align: center;
        }
        .result-box {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 8px;
            border-left: 4px solid #2E8B57;
            margin: 0.5rem 0;
        }
        .arabic-text {
            direction: rtl;
            text-align: right;
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
        }
        .stButton > button {
            background: linear-gradient(135deg, #2E8B57 0%, #228B22 100%);
            color: white;
            border: none;
            border-radius: 8px;
            padding: 0.5rem 1.5rem;
            width: 100%;
        }
        .fast-stats {
            background: #e8f5e8;
            padding: 0.5rem;
            border-radius: 5px;
            font-size: 0.9em;
            margin: 0.5rem 0;
        }
        .accuracy-indicator {
            background: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
            padding: 0.75rem;
            border-radius: 8px;
            margin: 0.5rem 0;
        }
    </style>
    """, unsafe_allow_html=True)

@st.cache_resource(show_spinner=False)
def get_vqa_system():
    """Get cached VQA system"""
    return AccurateMedicalVQA()

def validate_file(uploaded_file) -> tuple:
    """Quick file validation"""
    if not uploaded_file:
        return False, "No file uploaded"
    
    if uploaded_file.size > MAX_FILE_SIZE:
        return False, "File too large (max 5MB)"
    
    ext = uploaded_file.name.split('.')[-1].lower()
    if ext not in SUPPORTED_FORMATS:
        return False, f"Use: {', '.join(SUPPORTED_FORMATS)}"
    
    return True, "Valid file"

def main():
    """Main application"""
    init_app()
    apply_theme()
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ©º Accurate Medical AI Assistant</h1>
        <p><strong>Enhanced for Precision - Advanced Medical Image Analysis</strong></p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialize system
    vqa_system = get_vqa_system()
    
    # Load model
    if vqa_system.model is None:
        with st.spinner("ğŸ”„ Loading enhanced medical model..."):
            success = vqa_system.load_model()
            if success:
                st.success("âœ… Advanced medical model loaded successfully!")
                st.balloons()
            else:
                st.error("âŒ Model loading failed")
                st.stop()
    
    # Main interface
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("### ğŸ“¤ Upload Medical Image")
        
        uploaded_file = st.file_uploader(
            "Choose medical image (max 5MB):",
            type=SUPPORTED_FORMATS,
            help="Supported: JPG, JPEG, PNG"
        )
        
        if uploaded_file:
            is_valid, message = validate_file(uploaded_file)
            
            if is_valid:
                try:
                    image = Image.open(uploaded_file)
                    st.image(image, caption=uploaded_file.name, use_container_width=True)
                    st.info(f"ğŸ“Š Size: {image.size[0]}Ã—{image.size[1]}")
                except Exception as e:
                    st.error(f"âŒ Image error: {str(e)}")
                    uploaded_file = None
            else:
                st.error(f"âŒ {message}")
                uploaded_file = None
    
    with col2:
        st.markdown("### ğŸ’­ Ask Medical Question")
        
        # Language selector
        language = st.selectbox(
            "Language:",
            options=["ar", "en"],
            format_func=lambda x: "ğŸ‡ªğŸ‡¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" if x == "ar" else "ğŸ‡ºğŸ‡¸ English"
        )
        
        # Question input
        if language == "ar":
            placeholder = "Ù…Ø§ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©ØŸ Ø£Ùˆ ØµÙ Ù…Ø§ ØªØ±Ø§Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©"
            label = "Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠ:"
        else:
            placeholder = "What is the likely diagnosis? Or describe what you see in the image"
            label = "Medical Question:"
        
        question = st.text_area(
            label,
            height=100,
            placeholder=placeholder
        )
        
        # Analyze button
        if st.button("ğŸ” Accurate Analysis"):
            if not uploaded_file:
                st.warning("âš ï¸ Upload image first")
            elif not question.strip():
                st.warning("âš ï¸ Enter question")
            else:
                with st.spinner("ğŸ§  Analyzing with enhanced AI..."):
                    try:
                        image = Image.open(uploaded_file)
                        result = vqa_system.process_query(image, question)
                        
                        if result["success"]:
                            st.markdown("---")
                            st.markdown("### ğŸ¯ Accurate Medical Analysis")
                            
                            # Processing time and accuracy indicator
                            st.markdown(f"""
                            <div class="accuracy-indicator">
                                âœ… <strong>Enhanced Analysis Complete</strong> | 
                                â±ï¸ <strong>{result['processing_time']:.2f}s</strong> | 
                                ğŸ” <strong>{'Arabic' if result['detected_language'] == 'ar' else 'English'}</strong>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # Results
                            res_col1, res_col2 = st.columns(2)
                            
                            with res_col1:
                                st.markdown("**ğŸ‡ºğŸ‡¸ English Analysis**")
                                st.markdown(f"**Q:** {result['question']}")
                                st.markdown(f"**Medical Finding:** {result['answer_en']}")
                            
                            with res_col2:
                                st.markdown("**ğŸ‡ªğŸ‡¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ø¨ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**")
                                st.markdown(f"""
                                <div class="arabic-text">
                                    <strong>Ø§Ù„Ø³Ø¤Ø§Ù„:</strong> {result['question']}<br><br>
                                    <strong>Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø·Ø¨ÙŠØ©:</strong> {result['answer_ar']}
                                </div>
                                """, unsafe_allow_html=True)
                            
                            # Medical disclaimer
                            st.warning("âš ï¸ **Ù„Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙÙ‚Ø· - Ø§Ø³ØªØ´Ø± Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ Ù„Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ**")
                            
                        else:
                            st.error(f"âŒ Analysis failed: {result.get('error', 'Unknown')}")
                    
                    except Exception as e:
                        st.error(f"âŒ Processing error: {str(e)}")
    
    # Sidebar
    with st.sidebar:
        st.markdown("### ğŸ“Š System Status")
        
        if vqa_system.model is not None:
            st.success("âœ… Model: Ready")
            st.info(f"ğŸ–¥ï¸ Device: {vqa_system.device.upper()}")
            st.success("ğŸ¯ Enhanced Accuracy Mode")
        else:
            st.error("âŒ Model: Not Ready")
        
        st.markdown("---")
        st.markdown("""
        **ğŸ¯ Accuracy Features:**
        - âœ… Enhanced medical prompts
        - âœ… Advanced response generation
        - âœ… Comprehensive Arabic translation
        - âœ… Medical context awareness
        
        **ğŸ“‹ Best Practices:**
        1. Upload clear medical images
        2. Ask specific questions
        3. Use medical terminology
        4. Specify body parts/regions
        
        **ğŸ©º Supported Analysis:**
        - X-rays, CT scans, MRI
        - Chest, brain, abdomen imaging
        - Bone fractures, infections
        - Normal vs abnormal findings
        """)
        
        st.markdown("---")
        st.markdown("**âš ï¸ Medical Disclaimer**")
        st.caption("This AI provides preliminary analysis for educational purposes. Always consult qualified healthcare professionals for medical diagnosis and treatment decisions.")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        <p><strong>Accurate Medical VQA v2.0</strong> | Enhanced Precision & Arabic Support</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
