import streamlit as st
import torch
from transformers import LlavaProcessor, LlavaForConditionalGeneration, MarianTokenizer, MarianMTModel
from PIL import Image
import time
from typing import Dict, List, Tuple

# Configure Streamlit page
st.set_page_config(
    page_title="Medical VQA Chatbot",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #2E86AB;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        max-width: 80%;
    }
    .user-message {
        background-color: #DCF8C6;
        margin-left: auto;
        text-align: right;
    }
    .bot-message {
        background-color: #F1F1F1;
        margin-right: auto;
    }
    .arabic-text {
        font-family: 'Arial', sans-serif;
        direction: rtl;
        text-align: right;
    }
    .stButton > button {
        width: 100%;
        background-color: #2E86AB;
        color: white;
    }
    .upload-section {
        border: 2px dashed #2E86AB;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

class MedicalVQAChatbot:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models_loaded = False
        self.load_models()
    
    @st.cache_resource
    def load_models(_self):
        """Load all required models with caching"""
        try:
            with st.spinner("Loading Medical VQA Model... This may take a few minutes on first run."):
                _self.vqa_processor = LlavaProcessor.from_pretrained("ButterflyCatGirl/llava-medical-VQA-lora-merged3")
                _self.vqa_model = LlavaForConditionalGeneration.from_pretrained(
                    "Mohamed264/llava-medical-VQA-lora-merged3",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
                if not torch.cuda.is_available():
                    _self.vqa_model.to(_self.device)
            
            with st.spinner("Loading Translation Models..."):
                _self.en_ar_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
                _self.en_ar_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
                _self.ar_en_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
                _self.ar_en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
            
            _self.models_loaded = True
            st.success("âœ… All models loaded successfully!")
            return True
            
        except Exception as e:
            st.error(f"âŒ Error loading models: {str(e)}")
            return False
    
    def detect_language(self, text: str) -> str:
        """Detect if text is Arabic or English"""
        arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or '\u0750' <= char <= '\u077F')
        english_chars = sum(1 for char in text.lower() if 'a' <= char <= 'z')
        return 'ar' if arabic_chars > english_chars else 'en'
    
    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """Translate text between Arabic and English"""
        if source_lang == target_lang:
            return text
        
        try:
            if source_lang == 'ar' and target_lang == 'en':
                inputs = self.ar_en_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                with torch.no_grad():
                    translated = self.ar_en_model.generate(**inputs, max_length=512, num_beams=4)
                translated_text = self.ar_en_tokenizer.decode(translated[0], skip_special_tokens=True)
                
            elif source_lang == 'en' and target_lang == 'ar':
                inputs = self.en_ar_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                with torch.no_grad():
                    translated = self.en_ar_model.generate(**inputs, max_length=512, num_beams=4)
                translated_text = self.en_ar_tokenizer.decode(translated[0], skip_special_tokens=True)
            else:
                return text
                
            return translated_text.strip()
            
        except Exception as e:
            st.error(f"Translation error: {str(e)}")
            return text
    
    def process_medical_vqa(self, image: Image.Image, question: str) -> Tuple[str, str]:
        """Process medical VQA with image and question"""
        try:
            input_lang = self.detect_language(question)
            english_question = self.translate_text(question, input_lang, 'en') if input_lang == 'ar' else question
            
            prompt = f"USER: <image>\n{english_question}\nASSISTANT:"
            inputs = self.vqa_processor(text=prompt, images=image, return_tensors="pt")
            
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.vqa_model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.vqa_model.config.pad_token_id
                )
            
            full_response = self.vqa_processor.decode(outputs[0], skip_special_tokens=True)
            english_response = full_response.split("ASSISTANT:")[-1].strip() if "ASSISTANT:" in full_response else full_response.strip()
            english_response = english_response.replace("USER:", "").replace("<image>", "").strip()
            
            final_response = self.translate_text(english_response, 'en', input_lang) if input_lang == 'ar' and english_response else english_response
            return final_response, input_lang
            
        except Exception as e:
            error_msg = f"Error processing VQA: {str(e)}"
            st.error(error_msg)
            return error_msg, 'en'

def initialize_session_state():
    """Initialize session state variables"""
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = MedicalVQAChatbot()
    if 'uploaded_image' not in st.session_state:
        st.session_state.uploaded_image = None

def display_chat_message(message: Dict, is_user: bool = False):
    """Display a chat message with proper styling"""
    css_class = "user-message" if is_user else "bot-message"
    lang_class = "arabic-text" if message.get('language') == 'ar' else ""
    
    st.markdown(f"""
    <div class="chat-message {css_class} {lang_class}">
        <strong>{'You' if is_user else 'Medical Assistant'}:</strong><br>
        {message['content']}
        <br><small>{message['timestamp']}</small>
    </div>
    """, unsafe_allow_html=True)

def main():
    initialize_session_state()
    
    st.markdown("<h1 class='main-header'>ğŸ¥ Medical VQA Chatbot</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center; color: #666;'>Upload medical images and ask questions in English or Arabic | Ø§Ø±ÙØ¹ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© ÙˆØ§Ø³Ø£Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</p>", unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("ğŸ“‹ Instructions | Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª")
        st.markdown("""
        **English:**
        1. Upload a medical image
        2. Ask your question
        3. Get AI-powered analysis
        
        **Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**
        1. Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ©
        2. Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ùƒ
        3. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        """)
        
        st.header("ğŸ”§ System Status")
        if st.session_state.chatbot.models_loaded:
            st.success("âœ… Models Ready")
            st.info(f"Device: {st.session_state.chatbot.device}")
        else:
            st.error("âŒ Models Not Loaded")
        
        if st.button("ğŸ—‘ï¸ Clear Chat | Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st.session_state.chat_history = []
            st.session_state.uploaded_image = None
            st.rerun()
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ’¬ Chat History | Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        chat_container = st.container()
        with chat_container:
            if st.session_state.chat_history:
                for message in st.session_state.chat_history:
                    display_chat_message(message, message['sender'] == 'user')
            else:
                st.info("Start a conversation by uploading an image and asking a question!")
    
    with col2:
        st.subheader("ğŸ“¤ Upload Image | Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©")
        uploaded_file = st.file_uploader(
            "Choose a medical image | Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ©",
            type=['png', 'jpg', 'jpeg', 'bmp', 'tiff'],
            help="Supported formats: PNG, JPG, JPEG, BMP, TIFF"
        )
        
        if uploaded_file is not None:
            try:
                image = Image.open(uploaded_file).convert("RGB")
                st.image(image, caption="Uploaded Image | Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", use_column_width=True)
                st.session_state.uploaded_image = image
                st.success("âœ… Image uploaded successfully! | ØªÙ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­!")
            except Exception as e:
                st.error(f"Error loading image: {str(e)}")
    
    st.subheader("â“ Ask Your Question | Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ")
    col_q1, col_q2 = st.columns([4, 1])
    
    with col_q1:
        user_question = st.text_area(
            "Enter your question in English or Arabic:",
            placeholder="What abnormality is visible in this X-ray? | Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…Ø±Ø¦ÙŠ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø³ÙŠÙ†ÙŠØ©ØŸ",
            height=100,
            key="question_input"
        )
    
    with col_q2:
        send_button = st.button("ğŸš€ Send | Ø¥Ø±Ø³Ø§Ù„", type="primary", use_container_width=True)
    
    if send_button and user_question.strip():
        if st.session_state.uploaded_image is None:
            st.warning("âš ï¸ Please upload an image first! | ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£ÙˆÙ„Ø§Ù‹!")
        elif not st.session_state.chatbot.models_loaded:
            st.error("âŒ Models are not loaded. Please refresh the page. | Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ù…Ø­Ù…Ù„Ø©. ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø©.")
        else:
            with st.spinner("Processing your question... | Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ..."):
                user_message = {
                    'content': user_question,
                    'sender': 'user',
                    'timestamp': time.strftime("%H:%M:%S"),
                    'language': st.session_state.chatbot.detect_language(user_question)
                }
                st.session_state.chat_history.append(user_message)
                
                response, detected_lang = st.session_state.chatbot.process_medical_vqa(
                    st.session_state.uploaded_image,
                    user_question
                )
                
                bot_message = {
                    'content': response,
                    'sender': 'bot',
                    'timestamp': time.strftime("%H:%M:%S"),
                    'language': detected_lang
                }
                st.session_state.chat_history.append(bot_message)
                
                # Clear the input by resetting the key
                st.session_state.question_input = ""
                
            st.rerun()
    
    st.subheader("ğŸ’¡ Example Questions | Ø£Ø³Ø¦Ù„Ø© Ù†Ù…ÙˆØ°Ø¬ÙŠØ©")
    col_ex1, col_ex2 = st.columns(2)
    
    with col_ex1:
        st.markdown("""
        **English Examples:**
        - What abnormality is visible in this X-ray?
        - Describe the pathological findings in this image
        - What is the diagnosis based on this medical image?
        - Are there any signs of infection or inflammation?
        """)
    
    with col_ex2:
        st.markdown("""
        **Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**
        - Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ù…Ø±Ø¦ÙŠ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø³ÙŠÙ†ÙŠØ©ØŸ
        - ØµÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±Ø¶ÙŠØ© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©
        - Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø·Ø¨ÙŠØ©ØŸ
        - Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø¹Ù„Ø§Ù…Ø§Øª Ø¹Ø¯ÙˆÙ‰ Ø£Ùˆ Ø§Ù„ØªÙ‡Ø§Ø¨ØŸ
        """)
    
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; font-size: 0.8em;'>
        ğŸ¥ Medical VQA Chatbot | Powered by LLaVA Medical VQA Model<br>
        âš ï¸ <strong>Disclaimer:</strong> This tool is for educational purposes only. Always consult healthcare professionals for medical advice.
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
