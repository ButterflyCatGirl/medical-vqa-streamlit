import streamlit as st
from transformers import BlipProcessor, BlipForQuestionAnswering, MarianMTModel, MarianTokenizer
from PIL import Image
import torch

# Cache model and processor loading for performance
@st.cache_resource
def load_blip_model():
    return BlipForQuestionAnswering.from_pretrained("sharawy53/diploma")

@st.cache_resource
def load_blip_processor():
    return BlipProcessor.from_pretrained("sharawy53/diploma")

@st.cache_resource
def load_ar_en_tokenizer():
    return MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ar-en")

@st.cache_resource
def load_ar_en_model():
    return MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-ar-en")

@st.cache_resource
def load_en_ar_tokenizer():
    return MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")

@st.cache_resource
def load_en_ar_model():
    return MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar")

# Load models and processors
blip_model = load_blip_model()
processor = load_blip_processor()
ar_en_tokenizer = load_ar_en_tokenizer()
ar_en_model = load_ar_en_model()
en_ar_tokenizer = load_en_ar_tokenizer()
en_ar_model = load_en_ar_model()

# Translation functions
def translate_ar_to_en(text):
    inputs = ar_en_tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = ar_en_model.generate(**inputs)
    return ar_en_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

def translate_en_to_ar(text):
    inputs = en_ar_tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = en_ar_model.generate(**inputs)
    return en_ar_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# Manual medical terms dictionary
medical_terms = {
    "chest x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ© Ù„Ù„ØµØ¯Ø±",
    "x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ©",
    "ct scan": "ØªØµÙˆÙŠØ± Ù…Ù‚Ø·Ø¹ÙŠ Ù…Ø­ÙˆØ³Ø¨",
    "mri": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ",
    "ultrasound": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ù…ÙˆØ¬Ø§Øª ÙÙˆÙ‚ Ø§Ù„ØµÙˆØªÙŠØ©",
    "normal": "Ø·Ø¨ÙŠØ¹ÙŠ",
    "abnormal": "ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ",
    "brain": "Ø§Ù„Ø¯Ù…Ø§Øº",
    "fracture": "ÙƒØ³Ø±",
    "no abnormality detected": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø´Ø°ÙˆØ°Ø§Øª",
    "left lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠØ³Ø±Ù‰",
    "right lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠÙ…Ù†Ù‰"
}

def translate_answer_medical(answer_en):
    key = answer_en.lower().strip()
    if key in medical_terms:
        return medical_terms[key]
    else:
        return translate_en_to_ar(answer_en)

# Main VQA function
def vqa_multilingual(image, question):
    if not image or not question.strip():
        return "ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„.", "", "", ""

    is_arabic = any('\u0600' <= c <= '\u06FF' for c in question)
    if is_arabic:
        question_ar = question.strip()
        question_en = translate_ar_to_en(question_ar)
    else:
        question_en = question.strip()
        question_ar = translate_en_to_ar(question_en)

    inputs = processor(image, question_en, return_tensors="pt")
    with torch.no_grad():
        output = blip_model.generate(**inputs)
    answer_en = processor.decode(output[0], skip_special_tokens=True).strip()
    answer_ar = translate_answer_medical(answer_en)

    return (
        f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n{question_ar}",
        f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:\n{question_en}",
        f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n{answer_ar}",
        f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:\n{answer_en}"
    )

# Streamlit interface
st.title("Ù†Ù…ÙˆØ°Ø¬ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© (Ø¹Ø±Ø¨ÙŠ - Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ) Ù„ØµÙˆØ± Ø§Ù„Ø£Ø´Ø¹Ø©")
st.write("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ© ÙˆØ§Ø³Ø£Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ÙˆØ³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØªÙŠÙ†.")

uploaded_image = st.file_uploader("ğŸ” Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø£Ø´Ø¹Ø©", type=["jpg", "png", "jpeg"])
question = st.text_input("ğŸ’¬ Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ (Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)")

if st.button("Submit"):
    if uploaded_image is not None and question.strip():
        with st.spinner("Ø¬Ø§Ø±Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©..."):
            image = Image.open(uploaded_image)
            result = vqa_multilingual(image, question)
        st.write("ğŸŸ  Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:")
        st.write(result[0])
        st.write("ğŸŸ¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:")
        st.write(result[1])
        st.write("ğŸŸ  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:")
        st.write(result[2])
        st.write("ğŸŸ¢ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:")
        st.write(result[3])
    else:
        st.write("ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„.")
