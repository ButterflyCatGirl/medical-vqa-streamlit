# app.py
import streamlit as st
from transformers import BlipProcessor, BlipForQuestionAnswering
from PIL import Image
import torch
import sys

# Set page config
st.set_page_config(
    page_title="Ù†Ù…ÙˆØ°Ø¬ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© Ù„ØµÙˆØ± Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø·Ø¨ÙŠØ©",
    page_icon="ğŸ©»",
    layout="centered"
)

# Check for required libraries
try:
    from transformers import MarianMTModel, MarianTokenizer
except ImportError:
    st.error("""
    **Missing required dependencies!**  
    Please add `sentencepiece` to your requirements.txt file:
    ```
    sentencepiece==0.2.0
    ```
    """)
    st.stop()

# Load models with caching and error handling
@st.cache_resource(show_spinner="Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬... (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚)")
def load_models():
    try:
        # Load BLIP VQA model
        blip_model = BlipForQuestionAnswering.from_pretrained("sharawy53/diploma")
        processor = BlipProcessor.from_pretrained("sharawy53/diploma")
        
        # Load translation models
        ar_en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
        ar_en_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
        
        en_ar_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
        en_ar_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
        
        return {
            "blip": (blip_model, processor),
            "ar_en": (ar_en_model, ar_en_tokenizer),
            "en_ar": (en_ar_model, en_ar_tokenizer)
        }
    except Exception as e:
        st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")
        st.error("""
        **Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:**
        1. Ù…Ø³Ø§Ø­Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† ØºÙŠØ± ÙƒØ§ÙÙŠØ© (Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØ­ØªØ§Ø¬ ~1.5GB)
        2. Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
        3. Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø®Ø§Ø¯Ù… Hugging Face
        """)
        st.stop()

models = load_models()

# Medical terms dictionary
medical_terms = {
    "chest x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ© Ù„Ù„ØµØ¯Ø±",
    "x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ©",
    "ct scan": "ØªØµÙˆÙŠØ± Ù…Ù‚Ø·Ø¹ÙŠ Ù…Ø­ÙˆØ³Ø¨",
    "mri": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ",
    "ultrasound": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ù…ÙˆØ¬Ø§Øª ÙÙˆÙ‚ Ø§Ù„ØµÙˆØªÙŠØ©",
    "normal": "Ø·Ø¨ÙŠØ¹ÙŠ",
    "abnormal": "ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ",
    "brain": "Ø§Ù„Ø¯Ù…Ø§Øº",
    "fracture": "ÙƒØ³Ø±",
    "no abnormality detected": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø´Ø°ÙˆØ°Ø§Øª",
    "left lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠØ³Ø±Ù‰",
    "right lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠÙ…Ù†Ù‰",
    "heart": "Ø§Ù„Ù‚Ù„Ø¨",
    "lungs": "Ø§Ù„Ø±Ø¦ØªÙŠÙ†",
    "spine": "Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ"
}

# Translation functions
def translate_ar_to_en(text):
    model, tokenizer = models["ar_en"]
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

def translate_en_to_ar(text):
    model, tokenizer = models["en_ar"]
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

def translate_answer_medical(answer_en):
    key = answer_en.lower().strip()
    return medical_terms.get(key, translate_en_to_ar(answer_en))

# Main VQA function
def vqa_multilingual(image, question):
    if not image or not question.strip():
        return None
    
    # Detect language
    is_arabic = any('\u0600' <= c <= '\u06FF' for c in question)
    
    try:
        if is_arabic:
            question_ar = question.strip()
            question_en = translate_ar_to_en(question_ar)
        else:
            question_en = question.strip()
            question_ar = translate_en_to_ar(question_en)

        # Process image and question
        blip_model, processor = models["blip"]
        inputs = processor(image, question_en, return_tensors="pt")
        
        with torch.no_grad():
            output = blip_model.generate(**inputs)
        
        answer_en = processor.decode(output[0], skip_special_tokens=True).strip()
        answer_ar = translate_answer_medical(answer_en)
        
        return {
            "arabic_question": question_ar,
            "english_question": question_en,
            "arabic_answer": answer_ar,
            "english_answer": answer_en
        }
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")
        return None

# UI Components
st.title("ğŸ§  Ù†Ù…ÙˆØ°Ø¬ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ) Ù„ØªØ­Ù„ÙŠÙ„ ØµÙˆØ± Ø§Ù„Ø£Ø´Ø¹Ø©")
st.markdown("""
Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ© (Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ©ØŒ Ù…Ù‚Ø·Ø¹ÙŠØ©ØŒ Ø±Ù†ÙŠÙ† Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ) ÙˆØ§Ø³Ø£Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
""")

with st.form("vqa_form"):
    col1, col2 = st.columns(2)
    with col1:
        uploaded_image = st.file_uploader("ğŸ“· Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø£Ø´Ø¹Ø©", type=["jpg", "jpeg", "png"])
    with col2:
        question_input = st.text_input("â“ Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)", "")
    
    submitted = st.form_submit_button("ğŸš€ Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø©")

if submitted:
    if not uploaded_image:
        st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£ÙˆÙ„Ø§Ù‹")
        st.stop()
        
    if not question_input.strip():
        st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„")
        st.stop()
        
    image = Image.open(uploaded_image)
    with st.spinner("Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„... (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ 10-20 Ø«Ø§Ù†ÙŠØ©)"):
        results = vqa_multilingual(image, question_input)
    
    if results:
        st.success("âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", width=300)
        
        st.subheader("ğŸ—¨ï¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ğŸŸ  Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**")
            st.info(results["arabic_question"])
            
            st.markdown("**ğŸŸ¢ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©**")
            st.success(results["english_answer"])
            
        with col2:
            st.markdown("**ğŸŸ¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©**")
            st.info(results["english_question"])
            
            st.markdown("**ğŸŸ  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**")
            st.success(results["arabic_answer"])
    else:
        st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰")
